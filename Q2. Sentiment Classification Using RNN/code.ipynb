{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f954862",
   "metadata": {},
   "source": [
    "Q2: Sentiment Classification Using RNN\n",
    "\n",
    "Task: Sentiment analysis determines if a given text expresses a positive or negative emotion. You will train an LSTM-based sentiment classifier using the IMDB dataset.\n",
    "\n",
    "1.\tLoad the IMDB sentiment dataset (tensorflow.keras.datasets.imdb).\n",
    "\n",
    "2.\tPreprocess the text data by tokenization and padding sequences.\n",
    "\n",
    "3.\tTrain an LSTM-based model to classify reviews as positive or negative.\n",
    "\n",
    "4.\tGenerate a confusion matrix and classification report (accuracy, precision, recall, F1-score).\n",
    "\n",
    "5.\tInterpret why precision-recall tradeoff is important in sentiment classification.\n",
    "\n",
    "Hint: Use confusion_matrix and classification_report from sklearn.metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe80b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "# Load top 10,000 most frequent words\n",
    "vocab_size = 10000\n",
    "# tokenization is already done by load_data\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c046e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "maxlen = 500  # max length of review\n",
    "# padding to ensure all reviews are the same length\n",
    "x_train_padded = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_padded = pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 225ms/step - accuracy: 0.6689 - loss: 0.5807 - val_accuracy: 0.8370 - val_loss: 0.3850\n",
      "Epoch 2/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 244ms/step - accuracy: 0.8869 - loss: 0.2809 - val_accuracy: 0.8634 - val_loss: 0.3175\n",
      "Epoch 3/3\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 257ms/step - accuracy: 0.9328 - loss: 0.1797 - val_accuracy: 0.8678 - val_loss: 0.3166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x235b409b860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the model\n",
    "embedding_dim = 64\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')  # binary classification\n",
    "])\n",
    "\n",
    "# fit the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_padded, y_train, epochs=3, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c912441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step\n",
      "Confusion Matrix\n",
      " [[10926  1574]\n",
      " [ 1812 10688]]\n",
      "\n",
      "Classification Reoprt               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87     12500\n",
      "           1       0.87      0.86      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and convert to binary\n",
    "y_pred_prob = model.predict(x_test_padded)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86926ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[10926  1574]\n",
      " [ 1812 10688]]\n",
      "\n",
      "Classification Reoprt\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87     12500\n",
      "           1       0.87      0.86      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate metrics\n",
    "print(\"Confusion Matrix\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Reoprt\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761df5f1",
   "metadata": {},
   "source": [
    "5.\tInterpret why precision-recall tradeoff is important in sentiment classification.\n",
    "\n",
    "High precision and low recall causes the model to be more conservative with decaliring positives but you may miss out on some true positives. Whereas high recall and low precision captures more true positives but is more likely to false classify as positive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
