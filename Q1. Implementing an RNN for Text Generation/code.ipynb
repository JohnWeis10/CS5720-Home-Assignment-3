{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68c0403",
   "metadata": {},
   "source": [
    "Q1: Implementing an RNN for Text Generation\n",
    "\n",
    "Task: Recurrent Neural Networks (RNNs) can generate sequences of text. You will train an LSTM-based RNN to predict the next character in a given text dataset.\n",
    "\n",
    "1.\tLoad a text dataset (e.g., \"Shakespeare Sonnets\", \"The Little Prince\").\n",
    "\n",
    "2.\tConvert text into a sequence of characters (one-hot encoding or embeddings).\n",
    "\n",
    "3.\tDefine an RNN model using LSTM layers to predict the next character.\n",
    "\n",
    "4.\tTrain the model and generate new text by sampling characters one at a time.\n",
    "\n",
    "5.\tExplain the role of temperature scaling in text generation and its effect on randomness.\n",
    "\n",
    "Hint: Use tensorflow.keras.layers.LSTM() for sequence modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e30586e",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88170e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = 'https://www.gutenberg.org/files/100/100-0.txt'\n",
    "text = requests.get(url).text\n",
    "\n",
    "# Extract just the Sonnets section\n",
    "start = text.find(\"THE SONNETS\")\n",
    "end = text.find(\"THE END\", start)\n",
    "sonnets = text[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create mapping from chars to integers\n",
    "vocab = sorted(set(sonnets))\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "# Encode the full text as integer indices\n",
    "text_as_int = np.array([char2idx[c] for c in sonnets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f8f7a0",
   "metadata": {},
   "source": [
    "Creating Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4d65dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # input characters per training example\n",
    "examples_per_epoch = len(sonnets) // (seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# Create sequences of length seq_length + 1\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]   # First 100 chars\n",
    "    target_text = chunk[1:]   # Next 100 chars (shifted by one)\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951126a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e4b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defile RNN model with LSTM layers\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    # create an imput layer with fixed batch size\n",
    "    inputs = tf.keras.Input(batch_shape=(batch_size, None), dtype=tf.int32)\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = tf.keras.layers.LSTM(\n",
    "        rnn_units, return_sequences=True, stateful=True,\n",
    "        recurrent_initializer='glorot_uniform')(x)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "model = build_model(len(vocab), embedding_dim, rnn_units, batch_size=BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f3f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebace1",
   "metadata": {},
   "source": [
    "Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f57a43a",
   "metadata": {},
   "source": [
    "Use pretrained model weights to feed into inference model for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the inference model\n",
    "def build_inference_model(vocab_size, embedding_dim, rnn_units):\n",
    "    inputs = tf.keras.Input(batch_shape=(1, None), dtype=tf.int32)\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
    "    x = tf.keras.layers.LSTM(\n",
    "        rnn_units,\n",
    "        return_sequences=True,\n",
    "        stateful=True,\n",
    "        recurrent_initializer='glorot_uniform')(x)\n",
    "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "inference_model = build_inference_model(len(vocab), 256, 1024)\n",
    "\n",
    "# Set weights from the trained model\n",
    "inference_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68ef797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define how text generation is performed\n",
    "def reset_model_states(model):\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'reset_states'):\n",
    "            layer.reset_states()\n",
    "\n",
    "def generate_text(model, start_string, num_generate=500, temperature=1.0):\n",
    "    # Vectorize input\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "    reset_model_states(model)\n",
    "\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Adjust with temperature\n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        # Sample from the distribution\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Use predicted character as next input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d610abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall this,\n",
      "Slome love wayd Ingulim applouse, when thould wim, to truet,\n",
      "Andaine baiter my my candew and hese grom ance,\n",
      "\n",
      "\n",
      "S             10\n",
      "\n",
      "I and in my grest ‘ond murthreds:\n",
      "Aon thown with feme, wint my lozeast, bast, wild witt bling, con ghat I kelle\n",
      "Dy thrs tost thit to dought croemr’s rpovjing ale wand,\n",
      "Oot de reausth apt mavels walld now to muse.\n",
      "\n",
      "\n",
      "                     17\n",
      "\n",
      "Sine yothing oftile, where thate bads reaq me.\n",
      "\n",
      "\n",
      "                17\n",
      "\n",
      "\n",
      "Whand thate bus outing doth lite,\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(inference_model, start_string=\"Shall \",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d625369",
   "metadata": {},
   "source": [
    "5.\tExplain the role of temperature scaling in text generation and its effect on randomness.\n",
    "\n",
    "Randomness causes the model to pick the next character stochastically. Higher temperature causes less predictability and more creativity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
